from pydantic import BaseModel, Field, field_validator
from typing import Optional, List, Literal
from datetime import datetime
from enum import Enum

# Enums
class InterviewType(str, Enum):
    TECHNICAL = "technical"
    BEHAVIORAL = "behavioral"
    HR = "hr"
    SYSTEM_DESIGN = "system_design"

class ExperienceLevel(str, Enum):
    ENTRY = "entry"
    INTERMEDIATE = "intermediate"
    SENIOR = "senior"

class DifficultyLevel(str, Enum):
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"

# Request Models
class StartInterviewRequest(BaseModel):
    interview_type: InterviewType = Field(..., description="Type of interview")
    role: str = Field(..., min_length=2, max_length=100, description="Job role/position")
    experience_level: ExperienceLevel = Field(default=ExperienceLevel.INTERMEDIATE)
    domain: Optional[str] = Field(None, max_length=100, description="Specific domain/technology")

    @field_validator('role')
    @classmethod
    def validate_role(cls, v: str) -> str:
        return v.strip().title()

class SubmitAnswerRequest(BaseModel):
    session_id: str = Field(..., description="Interview session ID")
    question: str = Field(..., description="The question being answered")
    answer: str = Field(..., min_length=10, description="User's answer")
    
    @field_validator('answer')
    @classmethod
    def validate_answer(cls, v: str) -> str:
        if len(v.strip()) < 10:
            raise ValueError("Answer must be at least 10 characters")
        return v.strip()

class GetNextQuestionRequest(BaseModel):
    session_id: str
    previous_score: Optional[int] = None

# Response Models (Agent Output)
class QuestionResponse(BaseModel):
    """Structured question generated by the agent"""
    session_id: str
    question: str = Field(..., description="The interview question")
    context: str = Field(..., description="Context or hint about what to focus on")
    difficulty: DifficultyLevel
    expected_topics: List[str] = Field(default_factory=list, description="Topics that should be covered")
    time_limit_seconds: int = Field(default=180, description="Suggested time to answer")

class FeedbackDetail(BaseModel):
    """Detailed breakdown of feedback"""
    clarity: int = Field(..., ge=0, le=100, description="How clear and structured the answer was")
    technical_accuracy: int = Field(..., ge=0, le=100, description="Technical correctness")
    completeness: int = Field(..., ge=0, le=100, description="Coverage of expected topics")
    communication: int = Field(..., ge=0, le=100, description="Communication effectiveness")

class AnswerFeedback(BaseModel):
    """Structured feedback from the agent"""
    overall_score: int = Field(..., ge=0, le=100, description="Overall answer score")
    feedback_detail: FeedbackDetail
    strengths: List[str] = Field(..., min_length=1, description="What was done well")
    improvements: List[str] = Field(..., min_length=1, description="Areas to improve")
    missing_topics: List[str] = Field(default_factory=list, description="Topics that should have been mentioned")
    suggested_answer: str = Field(..., description="Example of a strong answer")
    follow_up_question: Optional[str] = Field(None, description="Natural follow-up question")

class InterviewSession(BaseModel):
    """Session tracking"""
    session_id: str
    interview_type: InterviewType
    role: str
    experience_level: ExperienceLevel
    questions_asked: int = 0
    average_score: float = 0.0
    created_at: datetime = Field(default_factory=datetime.now)
    
# Error Response
class ErrorResponse(BaseModel):
    error: str
    detail: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.now)

# Health Check
class HealthResponse(BaseModel):
    status: str
    service: str
    version: str
    agent_ready: bool
    timestamp: datetime = Field(default_factory=datetime.now)
